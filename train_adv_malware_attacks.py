
"""
train_adv_malware_attacks.py

This script performs a specialized version of adversarial training where adversarial
examples are generated ONLY from the malware samples within each batch.

The goal is to train the model intensively on malware evasion techniques while
still exposing it to clean benign samples.

Key Features:
-   For each training batch, it separates the benign and malware samples.
-   Adversarial examples are generated ONLY for the malware samples.
-   The model is then trained on a combination of:
    1. The original clean benign samples.
    2. The original clean malware samples.
    3. The newly created adversarial malware samples.
-   This creates a class imbalance in the training signal, which is a key
    characteristic of this experimental setup.
"""
import os
import json
import argparse
import logging
import torch
import numpy as np
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader

# --- CleverHans Imports ---
from cleverhans.torch.attacks.fast_gradient_method import fast_gradient_method
from cleverhans.torch.attacks.projected_gradient_descent import projected_gradient_descent

# Import custom modules
from data_loader import MalwareDataset
from model import MalwareDetectionModel
from train_adv import validate # Reuse the validation function

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def train_adversarial_malware_attacks(model, train_loader, optimizer, criterion, device, attack_fn, attack_kwargs):
    """
    Runs a single epoch of adversarial training, attacking only malware samples.
    """
    model.train()
    total_loss = 0
    correct_predictions = 0
    total_samples = 0

    for (api_batch, global_batch), labels_batch in train_loader:
        api_batch = api_batch.to(device)
        global_batch = global_batch.to(device)
        labels_batch = labels_batch.to(device)

        # --- 1. Separate Benign and Malware Samples ---
        is_malware = labels_batch == 1
        
        api_malware = api_batch[is_malware]
        global_malware = global_batch[is_malware]
        labels_malware = labels_batch[is_malware]
        
        # Continue only if there are malware samples in the batch
        if api_malware.shape[0] > 0:
            # --- 2. Generate Adversarial Examples for Malware ONLY ---
            def model_fn(x):
                # The global features must correspond to the malware samples
                probs_malware = model(x, global_malware)
                probs_benign = 1.0 - probs_malware
                return torch.stack([probs_benign, probs_malware], dim=-1)

            batch_attack_kwargs = attack_kwargs.copy()
            batch_attack_kwargs['y'] = labels_malware.long()
            
            api_malware_adv = attack_fn(model_fn, api_malware, **batch_attack_kwargs)

            # --- 3. Combine batches for training ---
            # We train on: clean original batch + adversarial malware
            combined_api = torch.cat([api_batch, api_malware_adv], dim=0)
            combined_global = torch.cat([global_batch, global_malware], dim=0)
            combined_labels = torch.cat([labels_batch, labels_malware], dim=0)
            
        else: # If no malware in batch, just use the clean batch
            combined_api, combined_global, combined_labels = api_batch, global_batch, labels_batch

        # --- 4. Forward pass, backward pass, and optimization ---
        optimizer.zero_grad()
        outputs = model(combined_api, combined_global)
        loss = criterion(outputs, combined_labels)
        loss.backward()
        optimizer.step()

        # --- 5. Statistics (based on the original clean batch performance) ---
        with torch.no_grad():
            outputs_clean = model(api_batch, global_batch)
        total_loss += criterion(outputs_clean, labels_batch).item() * api_batch.size(0)
        predicted = (outputs_clean > 0.5).float()
        correct_predictions += (predicted == labels_batch).sum().item()
        total_samples += labels_batch.size(0)

    avg_loss = total_loss / total_samples
    accuracy = correct_predictions / total_samples
    return avg_loss, accuracy

def main(args):
    """Main function to orchestrate the specialized adversarial training process."""
    
    device = torch.device("cuda" if torch.cuda.is_available() and not args.no_cuda else "cpu")
    logging.info(f"--- Starting Adversarial Training (Malware Attacks Only) ---")
    logging.info(f"Device: {device} | Attack: {args.attack.upper()} | Epsilon: {args.epsilon}")
    logging.info(f"Output will be saved to: {args.output_dir}")

    os.makedirs(args.output_dir, exist_ok=True)

    # --- Load Data ---
    splits_path = os.path.join(args.data_dir, 'dataset_splits.json')
    with open(splits_path, 'r') as f:
        splits = json.load(f)

    train_dataset = MalwareDataset(splits['train']['files'], splits['train']['labels'])
    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, drop_last=True)
    
    val_dataset = MalwareDataset(splits['validation']['files'], splits['validation']['labels'])
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)
    logging.info("Data loaders created successfully.")

    # --- Initialize Model, Optimizer, and Loss ---
    model = MalwareDetectionModel().to(device)
    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)
    criterion = nn.BCELoss()
    logging.info("Model, optimizer, and loss function initialized.")

    # --- Configure Attack ---
    if args.attack == 'fgsm':
        attack_fn = fast_gradient_method
        attack_kwargs = {'eps': args.epsilon, 'norm': np.inf}
    elif args.attack == 'pgd':
        attack_fn = projected_gradient_descent
        nb_iter = 10
        eps_iter = (2.5 * args.epsilon) / nb_iter
        attack_kwargs = {'eps': args.epsilon, 'eps_iter': eps_iter, 'nb_iter': nb_iter, 'norm': np.inf}
        logging.info(f"Configured PGD with: nb_iter={nb_iter}, eps_iter={eps_iter:.4f}")
    else:
        logging.error(f"Unknown attack type: {args.attack}")
        return
    
    # --- Training Loop ---
    best_val_accuracy = 0.0
    output_model_path = os.path.join(args.output_dir, 'best_model.pth')

    logging.info("Starting training loop...")
    for epoch in range(1, args.epochs + 1):
        train_loss, train_acc = train_adversarial_malware_attacks(model, train_loader, optimizer, criterion, device, attack_fn, attack_kwargs)
        val_loss, val_acc = validate(model, val_loader, criterion, device)

        logging.info(
            f"Epoch {epoch}/{args.epochs} | "
            f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | "
            f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}"
        )

        if val_acc > best_val_accuracy:
            best_val_accuracy = val_acc
            torch.save(model.state_dict(), output_model_path)
            logging.info(f"Validation accuracy improved. Saving model to {output_model_path}")

    logging.info("Adversarial training finished.")
    logging.info(f"Best validation accuracy achieved: {best_val_accuracy:.4f}")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Perform adversarial training attacking only MALWARE samples.")
    
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    parser.add_argument('--data_dir', type=str, default=script_dir,
                        help="Directory containing 'dataset_splits.json'.")
    parser.add_argument('--output_dir', type=str, required=True,
                        help="Directory to save the best adversarially trained model.")
    parser.add_argument('--attack', type=str, required=True, choices=['fgsm', 'pgd'],
                        help="The adversarial attack to train against.")
    parser.add_argument('--epsilon', type=float, required=True,
                        help="The perturbation magnitude (epsilon) for the attack.")
    
    parser.add_argument('--epochs', type=int, default=20, help="Number of training epochs.")
    parser.add_argument('--batch_size', type=int, default=64, help="Batch size for training.")
    parser.add_argument('--learning_rate', type=float, default=0.001, help="Learning rate.")
    parser.add_argument('--num_workers', type=int, default=2, help="Number of worker processes for DataLoader.")
    parser.add_argument('--no_cuda', action='store_true', help="Disable CUDA.")

    args = parser.parse_args()
    main(args)
