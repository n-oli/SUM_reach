
"""
evaluate_malware_attacks.py

This script performs a specialized evaluation for models that were trained to
defend against attacks on MALWARE samples only.

It mirrors the logic of `train_adv_malware_attacks.py` in its evaluation:
1.  It loads a trained model.
2.  For the clean data evaluation, it uses the full test set (benign and malware).
3.  For the adversarial evaluation (FGSM and PGD), it filters the test set
    and generates adversarial examples ONLY from the malware samples.
4.  The final adversarial accuracy is then calculated based on the model's
    performance on this subset of attacked malware and the original clean
    benign samples.
"""
import os
import json
import argparse
import logging
import torch
import numpy as np
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# --- CleverHans Imports ---
from cleverhans.torch.attacks.fast_gradient_method import fast_gradient_method
from cleverhans.torch.attacks.projected_gradient_descent import projected_gradient_descent

# Import custom modules
from data_loader import MalwareDataset
from model import MalwareDetectionModel

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def evaluate_clean(model, test_loader, device):
    """Evaluates model on a clean dataset and returns labels and predictions."""
    model.eval()
    all_labels = []
    all_predictions = []
    with torch.no_grad():
        for (api_batch, global_batch), labels in test_loader:
            api_batch, global_batch, labels = api_batch.to(device), global_batch.to(device), labels.to(device)
            outputs = model(api_batch, global_batch)
            predicted = (outputs > 0.5).float()
            all_labels.extend(labels.cpu().numpy())
            all_predictions.extend(predicted.cpu().numpy())
    return np.array(all_labels), np.array(all_predictions)

def evaluate_adversarial_malware_only(model, test_loader, attack_fn, attack_kwargs, device):
    """
    Evaluates model on an adversarial dataset where only malware is attacked.
    """
    model.eval()
    all_labels = []
    all_predictions = []
    
    for (api_batch, global_batch), labels in test_loader:
        api_batch, global_batch, labels = api_batch.to(device), global_batch.to(device), labels.to(device)

        # --- 1. Separate Benign and Malware Samples ---
        is_malware = labels == 1
        is_benign = labels == 0

        api_malware = api_batch[is_malware]
        global_malware = global_batch[is_malware]
        labels_malware = labels[is_malware]
        
        api_benign = api_batch[is_benign]
        global_benign = global_batch[is_benign]
        labels_benign = labels[is_benign]

        final_api_batch = api_benign
        final_global_batch = global_benign
        
        # --- 2. Generate Adversarial Examples for Malware ONLY ---
        if api_malware.shape[0] > 0:
            def model_fn(x):
                probs_malware = model(x, global_malware)
                probs_benign = 1.0 - probs_malware
                return torch.stack([probs_benign, probs_malware], dim=-1)

            batch_attack_kwargs = attack_kwargs.copy()
            batch_attack_kwargs['y'] = labels_malware.long()

            model.train()
            # Freeze BatchNorm layers to prevent running stats from being updated
            for module in model.modules():
                if isinstance(module, torch.nn.BatchNorm1d):
                    module.eval()
            api_malware_adv = attack_fn(model_fn, api_malware, **batch_attack_kwargs)
            model.eval()
            
            # Add the attacked malware to the clean benign samples
            final_api_batch = torch.cat([api_benign, api_malware_adv], dim=0)
            final_global_batch = torch.cat([global_benign, global_malware], dim=0)
            final_labels = torch.cat([labels_benign, labels_malware], dim=0)
        else: # No malware in this batch
            final_labels = labels_benign

        # --- 3. Evaluate on the combined batch ---
        if final_api_batch.shape[0] > 0:
            with torch.no_grad():
                outputs = model(final_api_batch, final_global_batch)
            predicted = (outputs > 0.5).float()
            all_labels.extend(final_labels.cpu().numpy())
            all_predictions.extend(predicted.cpu().numpy())

    return np.array(all_labels), np.array(all_predictions)


def plot_confusion_matrix(cm, class_names, title, output_filename):
    """Renders and saves the confusion matrix as a heatmap."""
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names)
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.title(title)
    plt.savefig(output_filename)
    plt.close()
    logging.info(f"Confusion matrix saved to {output_filename}")

def main(args):
    device = torch.device("cuda" if torch.cuda.is_available() and not args.no_cuda else "cpu")
    logging.info(f"Using device: {device}")

    # --- Load Data ---
    splits_path = os.path.join(args.data_dir, 'dataset_splits.json')
    with open(splits_path, 'r') as f:
        splits = json.load(f)
    test_dataset = MalwareDataset(splits['test']['files'], splits['test']['labels'])
    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, drop_last=True)
    logging.info(f"Test set loaded with {len(test_dataset)} samples.")

    # --- Load Model ---
    if not os.path.exists(args.model_path):
        logging.error(f"Model not found at {args.model_path}")
        return
        
    model_dir = os.path.dirname(args.model_path)
    model_name = os.path.basename(model_dir)
    if 'Neural_nets' in model_name: model_name = "Baseline (Clean)"
    
    # --- Create directory for plots ---
    plots_dir = os.path.join(model_dir, 'confusion_matrices')
    os.makedirs(plots_dir, exist_ok=True)
    
    logging.info(f"--- Evaluating model: {model_name} ---")
    
    model = MalwareDetectionModel().to(device)
    model.load_state_dict(torch.load(args.model_path, map_location=device))
    
    # --- Run Evaluations ---
    print(f"\n--- Evaluation Report for: {model_name} (Malware Attacks Only) ---")
    
    # 1. Clean Data Report
    print("\n[Clean Data Performance]")
    true_labels, pred_labels = evaluate_clean(model, test_loader, device)
    print(f"{'Accuracy':<12} | {accuracy_score(true_labels, pred_labels):.4f}")
    print(f"{'Precision':<12} | {precision_score(true_labels, pred_labels):.4f}")
    print(f"{'Recall':<12} | {recall_score(true_labels, pred_labels):.4f}")
    print(f"{'F1-Score':<12} | {f1_score(true_labels, pred_labels):.4f}")
    cm = confusion_matrix(true_labels, pred_labels)
    cm_path = os.path.join(plots_dir, 'confusion_matrix_clean.png')
    plot_confusion_matrix(cm, ['Benign', 'Malware'], f'Confusion Matrix (Clean Data)\n{model_name}', cm_path)
    
    # 2. Adversarial Robustness (Malware Attacks Only)
    print("\n[Adversarial Robustness (Malware Evasion)]")
    epsilons = [0.01, 0.03, 0.06, 0.08, 0.1]
    
    for eps in epsilons:
        print(f"\n--- FGSM (eps={eps}) ---")
        kwargs = {'eps': eps, 'norm': np.inf}
        true_labels_adv, pred_labels_adv = evaluate_adversarial_malware_only(model, test_loader, fast_gradient_method, kwargs, device)
        print(f"{'Accuracy':<12} | {accuracy_score(true_labels_adv, pred_labels_adv):.4f}")
        print(f"{'Precision':<12} | {precision_score(true_labels_adv, pred_labels_adv, zero_division=0):.4f}")
        print(f"{'Recall':<12} | {recall_score(true_labels_adv, pred_labels_adv, zero_division=0):.4f}")
        print(f"{'F1-Score':<12} | {f1_score(true_labels_adv, pred_labels_adv, zero_division=0):.4f}")
        cm_adv = confusion_matrix(true_labels_adv, pred_labels_adv)
        cm_adv_path = os.path.join(plots_dir, f'confusion_matrix_fgsm_malware_attack_{eps}.png')
        plot_confusion_matrix(cm_adv, ['Benign', 'Malware'], f'CM (FGSM Malware Attack \u03b5={eps})\n{model_name}', cm_adv_path)

    for eps in epsilons:
        print(f"\n--- PGD (eps={eps}) ---")
        kwargs = {'eps': eps, 'eps_iter': (2.5 * eps) / 40, 'nb_iter': 40, 'norm': np.inf}
        true_labels_adv, pred_labels_adv = evaluate_adversarial_malware_only(model, test_loader, projected_gradient_descent, kwargs, device)
        print(f"{'Accuracy':<12} | {accuracy_score(true_labels_adv, pred_labels_adv):.4f}")
        print(f"{'Precision':<12} | {precision_score(true_labels_adv, pred_labels_adv, zero_division=0):.4f}")
        print(f"{'Recall':<12} | {recall_score(true_labels_adv, pred_labels_adv, zero_division=0):.4f}")
        print(f"{'F1-Score':<12} | {f1_score(true_labels_adv, pred_labels_adv, zero_division=0):.4f}")
        cm_adv = confusion_matrix(true_labels_adv, pred_labels_adv)
        cm_adv_path = os.path.join(plots_dir, f'confusion_matrix_pgd_malware_attack_{eps}.png')
        plot_confusion_matrix(cm_adv, ['Benign', 'Malware'], f'CM (PGD Malware Attack \u03b5={eps})\n{model_name}', cm_adv_path)
        
    print("-------------------------------------------")

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Evaluate a single model's robustness, attacking only malware samples.")
    
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    parser.add_argument('--model_path', type=str, required=True, help="Path to the trained 'best_model.pth' file.")
    parser.add_argument('--data_dir', type=str, default=script_dir, help="Directory with dataset splits.")
    parser.add_argument('--batch_size', type=int, default=32, help="Batch size for evaluation.")
    parser.add_argument('--no_cuda', action='store_true', help="Disable CUDA.")

    args = parser.parse_args()
    main(args)
